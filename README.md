# papers

<!-- vim-markdown-toc GFM -->

* [NLP](#nlp)
	* [Text Matching](#text-matching)
	* [NER](#ner)
	* [Transformer](#transformer)

<!-- vim-markdown-toc -->

## NLP

### Text Matching

+ &spades; [Simple and Effective Text Matching with Richer Alignment Features](https://www.aclweb.org/anthology/P19-1465.pdf)
\[[Code](https://github.com/alibaba-edu/simple-effective-text-matching)\]

### NER

+ &spades; [Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning](https://www.aclweb.org/anthology/P19-1231.pdf)
\[[Code](https://github.com/v-mipeng/LexiconNER)\]

### Transformer

+ &spades; __BERT:__ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

+ &spades; __ERNIE 2.0:__ [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/pdf/1907.12412v1.pdf)
\[[Code](https://github.com/PaddlePaddle/ERNIE)\]

+ &spades; __RoBERTa:__ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692v1.pdf)
\[[Code](https://github.com/pytorch/fairseq/)\]

+ &spades; __XLNet:__ [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

+ &spades; __GPT-2:__ [Language Models are Unsupervised Multitask Learners](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)

+ &spades; __ALBERT:__ [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)

+ &spades; __T5:__ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
\[[Code](https://github.com/google-research/text-to-text-transfer-transformer)\]

+ &spades; __DistilBERT:__ [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)
