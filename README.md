# papers

<!-- vim-markdown-toc GFM -->

* [NLP](#nlp)
	* [Text Matching](#text-matching)
	* [NER](#ner)
	* [Transformer](#transformer)

<!-- vim-markdown-toc -->

## NLP

### Text Classification


+ &spades; [Dynamic Memory Induction Networks for Few-Shot Text Classification](https://arxiv.org/pdf/2005.05727.pdf)

### Text Matching

+ __Poly-encoders:__ [Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring](https://arxiv.org/pdf/1905.01969.pdf)
\[[Code](https://github.com/sfzhou5678/PolyEncoder)\]


+ __Sentence-BERT:__ [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)
\[[Code](https://github.com/UKPLab/sentence-transformers)\]

### NER

+ &spades; [Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning](https://www.aclweb.org/anthology/P19-1231.pdf)
\[[Code](https://github.com/v-mipeng/LexiconNER)\]

### Transformer

+ &spades; __BERT:__ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

+ &spades; __ERNIE 2.0:__ [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/pdf/1907.12412v1.pdf)
\[[Code](https://github.com/PaddlePaddle/ERNIE)\]

+ &spades; __RoBERTa:__ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692v1.pdf)
\[[Code](https://github.com/pytorch/fairseq/)\]

+ &spades; __XLNet:__ [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

+ &spades; __GPT-2:__ [Language Models are Unsupervised Multitask Learners](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)

+ &spades; __ALBERT:__ [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)

+ &spades; __T5:__ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
\[[code](https://github.com/google-research/text-to-text-transfer-transformer)\]

+ &spades; __DistilBERT:__ [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)


+ &spades; __MobileBERT:__ [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/pdf/2004.02984.pdf)
\[[code](https://github.com/google-research/google-research/tree/master/mobilebert)\]
