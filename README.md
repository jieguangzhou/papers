# papers

<!-- vim-markdown-toc GFM -->

* [NLP](#nlp)
	* [Text Classification](#text-classification)
	* [Text Matching](#text-matching)
	* [NER](#ner)
	* [Transformer](#transformer)
		* [压缩](#压缩)

<!-- vim-markdown-toc -->

## NLP

### Text Classification


+ &spades; [Dynamic Memory Induction Networks for Few-Shot Text Classification](https://arxiv.org/pdf/2005.05727.pdf)

### Text Matching

+ __Poly-encoders:__ [Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring](https://arxiv.org/pdf/1905.01969.pdf)
\[[Code](https://github.com/sfzhou5678/PolyEncoder)\]


+ __Sentence-BERT:__ [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)
\[[Code](https://github.com/UKPLab/sentence-transformers)\]

### NER

+ &spades; [Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning](https://www.aclweb.org/anthology/P19-1231.pdf)
\[[Code](https://github.com/v-mipeng/LexiconNER)\]

### Transformer

+ &spades; __BERT:__ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)


+ &spades; __ERNIE:__ [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)
\[[Code](https://github.com/PaddlePaddle/ERNIE)\]

+ &spades; __ERNIE 2.0:__ [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/pdf/1907.12412v1.pdf)
\[[Code](https://github.com/PaddlePaddle/ERNIE)\]

+ &spades; __RoBERTa:__ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692v1.pdf)
\[[Code](https://github.com/pytorch/fairseq/)\]

+ &spades; __ELECTRA:__ [ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS Kevin](https://arxiv.org/pdf/2003.10555.pdf) \[[Code](https://github.com/google-research/electra)\]

+ &spades; __XLNet:__ [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)


+ &spades; __Transformer-XL:__ [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Zihang](https://arxiv.org/pdf/1901.02860.pdf)
\[[Code](https://github.com/kimiyoung/transformer-xl)\]


+ &spades; __GPT:__ [Improving Language Understanding by Generative Pre-Training Alec](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

+ &spades; __GPT-2:__ [Language Models are Unsupervised Multitask Learners](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)

+ &spades; __GPT-3:__ [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)


+ &spades; __T5:__ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
\[[code](https://github.com/google-research/text-to-text-transfer-transformer)\]




#### 压缩

+ &spades; [Compressing Large-Scale Transformer-Based Models: A Case Study on BERT](https://arxiv.org/pdf/2002.11985.pdf)

+ &spades; __ALBERT:__ [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)


+ &spades; __TinyBERT:__ [TINYBERT: DISTILLING BERT FOR NATURAL LAN- GUAGE UNDERSTANDING](https://openreview.net/pdf?id=rJx0Q6EFPB)
\[[code](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)\]


+ &spades; __MobileBERT:__ [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/pdf/2004.02984.pdf)
\[[code](https://github.com/google-research/google-research/tree/master/mobilebert)\]

+ &spades; __DistilBERT:__ [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)

+ &spades; __FastBERT:__ [FastBERT: a Self-distilling BERT with Adaptive Inference Time Weijie](https://arxiv.org/pdf/2004.02178.pdf)
\[[code](https://github.com/autoliuweijie/FastBERT)\]


+ &spades; __Q-BERT:__ [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/pdf/1909.05840.pdf)


+ &spades; __Q8BERT:__ [Q8BERT: Quantized 8Bit BERT Ofir](https://arxiv.org/pdf/1910.06188.pdf)


+ &spades; __NEZHA:__ [NEZHA: NEURAL CONTEXTUALIZED REPRESENTATION FOR CHINESE LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1909.00204.pdf)

